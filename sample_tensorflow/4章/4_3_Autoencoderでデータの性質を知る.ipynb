{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os.path\n",
    "from scipy.stats import boxcox, norm\n",
    "\n",
    "from IPython.core.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set()\n",
    "\n",
    "tf.set_random_seed(123456789)\n",
    "np.random.seed(123456789)\n",
    "\n",
    "plt.rcParams['font.family'] = 'Yu Mincho'\n",
    "\n",
    "# C:\\\\sample\\\\auto_encoder にデータを読み書きする\n",
    "WORK_DIR = os.path.join('C:\\\\', 'sample', 'auto_encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ソースコード4.25　データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('データ読み込み開始')\n",
    "data = pd.read_pickle(\n",
    "    os.path.join(\n",
    "        WORK_DIR,\n",
    "        'data',\n",
    "        'excess_returns_with_financial_data.pickle'\n",
    "    )\n",
    ")\n",
    "print('データ読み込み終了')\n",
    "\n",
    "# 欠損データの割合を確認\n",
    "display(\n",
    "    (\n",
    "        data.isna().sum(axis=0) / data.shape[0]\n",
    "    ).sort_values(ascending=False)[:20]\n",
    ")\n",
    "\n",
    "data_without_na = data.select_dtypes(include='number').dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ソースコード4.26　各カラムのヒストグラムプロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 4, figsize=(20, 30))\n",
    "\n",
    "for column, ax in zip(data_without_na.columns, axes.flatten()):\n",
    "    sns.distplot(\n",
    "        data_without_na.loc[:, column],\n",
    "        ax=ax\n",
    "    ).set_title(\n",
    "        f'項目「{column}」のヒストグラム'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ソースコード4.27　カラム毎の標準化とボックス・コックス変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normed = (\n",
    "    data_without_na - data_without_na.min(axis=0)\n",
    ") / (\n",
    "    data_without_na.max(axis=0) - data_without_na.min(axis=0)\n",
    ")\n",
    "\n",
    "data_shifted = data_normed + data_normed.min(axis=0) + 1e-08\n",
    "data_bc = pd.DataFrame({\n",
    "    col: boxcox(\n",
    "        data_shifted.loc[:, col]\n",
    "    )[0] for col in data_shifted.columns\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ソースコード4.28　ボックス・コックス変換後のヒストグラムプロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 4, figsize=(20, 30))\n",
    "\n",
    "for column, ax in zip(data_bc.columns, axes.flatten()):\n",
    "    sns.distplot(\n",
    "        data_bc.loc[:, column].clip(\n",
    "            data_bc.loc[:, column].mean() - data_bc.loc[:, column].std()*3,\n",
    "            data_bc.loc[:, column].mean() + data_bc.loc[:, column].std()*3\n",
    "        ),\n",
    "        fit=norm,\n",
    "        ax=ax\n",
    "    ).set_title(\n",
    "        f'ボックスコックス変換後の項目「{column}」のヒストグラム'\n",
    "    )\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ソースコード4.29　ボックス・コックス変換後の標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bc_normed = (\n",
    "    data_bc - data_bc.min(axis=0)\n",
    ") / (\n",
    "    data_bc.max(axis=0) - data_bc.min(axis=0)\n",
    ") * 2 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ソースコード4.30　k-スパースオートエンコーダの構築（KSparseAutoEncoderクラスから抜粋）\n",
    "* ソースコード4.34　 エンコーディングとデコーディングメソッド（KSparseAutoEncoderクラスか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KSparseAutoEncoder:\n",
    "    \n",
    "# --- ソースコード4.30 ---\n",
    "    def __init__(\n",
    "        self,\n",
    "        sess,\n",
    "        num_columns,\n",
    "        k,\n",
    "        hidden_layer_dimension,\n",
    "        training=False\n",
    "    ):\n",
    "        with tf.variable_scope('simple_autoencoder'):\n",
    "            with tf.variable_scope('input_layer'):\n",
    "                self._inputs = tf.placeholder(\n",
    "                    tf.float32, [num_columns], name='inputs'\n",
    "                )\n",
    "                self._inputs_reshaped = tf.expand_dims(\n",
    "                    self._inputs,\n",
    "                    0\n",
    "                )\n",
    "            \n",
    "            # エンコーダ\n",
    "            with tf.variable_scope('encoder'):\n",
    "                self._weights = tf.get_variable(\n",
    "                    'weights',\n",
    "                    [num_columns, hidden_layer_dimension],\n",
    "                    dtype=tf.float32\n",
    "                )\n",
    "                self._encoder_bias = tf.get_variable(\n",
    "                    'encoder_bias',\n",
    "                    [hidden_layer_dimension],\n",
    "                    dtype=tf.float32\n",
    "                )\n",
    "                \n",
    "                self._hidden_layer = \\\n",
    "                self._inputs_reshaped @ self._weights + self._encoder_bias\n",
    "                \n",
    "                self._top_k_values, self.top_k_indices = tf.nn.top_k(\n",
    "                    self._hidden_layer, k=k\n",
    "                )\n",
    "                \n",
    "                self._encoding = tf.scatter_nd(\n",
    "                    indices=tf.expand_dims(\n",
    "                        tf.squeeze(self.top_k_indices), axis=-1\n",
    "                    ),\n",
    "                    updates=tf.squeeze(self._top_k_values),\n",
    "                    shape=tf.shape(tf.squeeze(self._hidden_layer))\n",
    "                )\n",
    "                \n",
    "            with tf.variable_scope('decoder'):\n",
    "                self._encoding_reshaped = tf.expand_dims(self._encoding, 0)\n",
    "                self._decoder_bias = tf.get_variable(\n",
    "                    'decoder_bias',\n",
    "                    [num_columns],\n",
    "                    dtype=tf.float32\n",
    "                )\n",
    "                \n",
    "                # ウェイトを使い回す\n",
    "                self._outputs = \\\n",
    "                self._encoding_reshaped @ tf.transpose(self._weights) + self._decoder_bias\n",
    "            \n",
    "            self._loss = tf.losses.mean_squared_error(\n",
    "                self._inputs_reshaped,\n",
    "                self._outputs\n",
    "            )\n",
    "            self._training = tf.train.AdamOptimizer().minimize(self._loss)\n",
    "            \n",
    "        self.saver = tf.train.Saver()\n",
    "        if training:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "# --- ソースコード4.30 ここまで ---\n",
    "            \n",
    "    def train_one_step(self, sess, inputs):\n",
    "        feed_dict = {\n",
    "            self._inputs: inputs\n",
    "        }\n",
    "        fetches = [\n",
    "            self._loss,\n",
    "            self._training\n",
    "        ]\n",
    "        return sess.run(fetches, feed_dict)\n",
    "\n",
    "# --- ソースコード4.34 ---\n",
    "    def encode(self, sess, inputs):\n",
    "        feed_dict = {self._inputs: inputs}\n",
    "        fetches = [self._encoding]\n",
    "        return sess.run(fetches, feed_dict)[0]\n",
    "    \n",
    "    def decode(self, sess, encoding):\n",
    "        feed_dict = {self._encoding: encoding}\n",
    "        fetches = [self._outputs]\n",
    "        return np.squeeze(sess.run(fetches, feed_dict)[0])\n",
    "# --- ソースコード4.34 ここまで ---\n",
    "    \n",
    "    def get_weights(self, sess):\n",
    "        fetches = [self._weights]\n",
    "        return sess.run(fetches)[0]\n",
    "    \n",
    "    def get_encoder_bias(self, sess):\n",
    "        fetches = [self._encoder_bias]\n",
    "        return sess.run(fetches)\n",
    "    \n",
    "    def get_decoder_bias(self, sess, inputs):\n",
    "        feed_dict = {self._inputs: inputs}\n",
    "        fetches = [self._decoder_bias]\n",
    "        return sess.run(fetches, feed_dict)\n",
    "    \n",
    "    def save(self, sess, path):\n",
    "        self.saver.save(sess, path)\n",
    "        print(f'model saved in {path}')\n",
    "        \n",
    "    def restore(self, sess, path):\n",
    "        self.saver.restore(sess, path)\n",
    "        print(f'model restored from {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ソースコード4.31　学習用メタパラメータの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 20\n",
    "num_columns = data_bc_normed.shape[1]\n",
    "max_training_steps = data_bc_normed.shape[0]\n",
    "steps_per_epoch = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ソースコード4.32　オートエンコーダの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_over_time = np.empty(max_training_steps // steps_per_epoch)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Graph().as_default() and tf.Session() as session:\n",
    "    auto_encoder = KSparseAutoEncoder(\n",
    "        session,\n",
    "        num_columns,\n",
    "        encoding_dim,\n",
    "        hidden_layer_dimension=num_columns * 4,\n",
    "        training=True\n",
    "    )\n",
    "    training_step = 0    \n",
    "    loss_mean_epoch = 0.\n",
    "    for random_row in np.random.choice(\n",
    "        data_bc_normed.shape[0],\n",
    "        max_training_steps,\n",
    "        replace=True  # 同じデータが数回選択されることを許す\n",
    "    ):\n",
    "        current_loss, _ = auto_encoder.train_one_step(\n",
    "            session,\n",
    "            data_bc_normed.iloc[random_row, :].values  # 期待出力は不要\n",
    "        )\n",
    "        loss_mean_epoch += current_loss\n",
    "        training_step += 1\n",
    "        if training_step % steps_per_epoch == 0:\n",
    "            loss_over_time[\n",
    "                training_step // steps_per_epoch - 1\n",
    "            ] = loss_mean_epoch / steps_per_epoch\n",
    "            \n",
    "            print(\n",
    "                f'[{(training_step * 100) // max_training_steps:3}%] '\n",
    "                f'直近{steps_per_epoch}ステップの平均ロス：'\n",
    "                f'{loss_over_time[training_step // steps_per_epoch - 1]:.5f}'\n",
    "            )\n",
    "            loss_mean_epoch = 0.\n",
    "            \n",
    "    auto_encoder.save(\n",
    "        session,\n",
    "        os.path.join(WORK_DIR, 'models', 'simple_auto_encoder', 'simple_auto_encoder'))\n",
    "loss_over_time_bkp = loss_over_time.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ソースコード4.33　ロスの時系列プロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(\n",
    "    loss_over_time_bkp, columns=['ロス']\n",
    ").rename_axis('学習エポック')\n",
    "loss_df = loss_df.assign(ロス移動平均=loss_df.rolling(10).mean())\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "grid = sns.lineplot(data=loss_df, ax=axes[0]).set_title(\n",
    "    'オートエンコーダ学習ロス時系列'\n",
    ")\n",
    "# plt.show()\n",
    "\n",
    "# _, ax = plt.subplots(figsize=(12, 5))\n",
    "sns.lineplot(data=loss_df[20:], ax=axes[1]).set_title(\n",
    "    'オートエンコーダ学習ロス時系列（20エポック目以降）'\n",
    ")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ソースコード4.35　入力とオートエンコーダの出力を比較するデータフレーム作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default() and tf.Session() as session:\n",
    "    auto_encoder = KSparseAutoEncoder(\n",
    "        session,\n",
    "        num_columns,\n",
    "        encoding_dim,\n",
    "        hidden_layer_dimension=num_columns * 4,\n",
    "        training=False\n",
    "    )\n",
    "    auto_encoder.restore(\n",
    "        session,\n",
    "        os.path.join(\n",
    "            WORK_DIR, 'models', 'simple_auto_encoder', 'simple_auto_encoder')\n",
    "    )\n",
    "    \n",
    "    row_i = np.random.randint(data_bc_normed.shape[0])\n",
    "    input_values = data_bc_normed.iloc[row_i, :].values\n",
    "    encoding = auto_encoder.encode(session, input_values)\n",
    "    decoded_encoding = auto_encoder.decode(session, encoding)\n",
    "\n",
    "inputs_and_reconstructed = pd.DataFrame(\n",
    "    data=np.transpose([input_values, decoded_encoding]),\n",
    "    columns=['入力', 'NN出力']\n",
    ").set_index(\n",
    "    data_bc_normed.columns\n",
    ").stack().reset_index()\n",
    "\n",
    "inputs_and_reconstructed.columns = ['カラム', '入力またはNN出力', '値']\n",
    "display(inputs_and_reconstructed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ソースコード4.36　入力値とオートエンコーダによって再現された値のプロット作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "sns.barplot(\n",
    "    ax=ax,\n",
    "    data=inputs_and_reconstructed,\n",
    "    x='カラム',\n",
    "    y='値',\n",
    "    hue='入力またはNN出力'\n",
    ").set_title(\n",
    "    '入力とそれに対してのオートエンコーダ出力（正規化、標準化後）'\n",
    ")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ソースコード4.37　オートエンコーダのウェイト行列のヒートマップ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default() and tf.Session() as session:\n",
    "    auto_encoder = KSparseAutoEncoder(\n",
    "        session,\n",
    "        num_columns,\n",
    "        encoding_dim,\n",
    "        hidden_layer_dimension=num_columns * 4,\n",
    "        training=False\n",
    "    )\n",
    "    auto_encoder.restore(\n",
    "        session,                \n",
    "        os.path.join(\n",
    "            WORK_DIR, 'models', 'simple_auto_encoder', 'simple_auto_encoder'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    weights = auto_encoder.get_weights(session)\n",
    "    \n",
    "    weights_df = pd.DataFrame(\n",
    "        weights,\n",
    "        columns=[f'変数{v}' for v in range(weights.shape[1])],\n",
    "        index=data_bc_normed.columns\n",
    "    )\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 12))\n",
    "sns.heatmap(weights_df, ax=ax, cmap='PiYG', center=0.)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
